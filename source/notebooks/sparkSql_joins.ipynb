{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.184.1:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1141c551780>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order_ID: string (nullable = true)\n",
      " |-- OrderDate: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub_Category: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantitty: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superStoreDF = spark.read.csv(\"file:///C:/data/SuperStore/SuperStore_US_Sales.csv\", header = True, inferSchema = True)\n",
    "superStoreDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "managerInfoDF = spark.read.csv(\"file:///C:/data/SuperStore/managerByRegion.csv\", header = True, inferSchema = True)\n",
    "managerInfoDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Returned: string (nullable = true)\n",
      " |-- Returned_Order_ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "returnInfoDF = spark.read.csv(\"file:///C:/data/SuperStore/totalReturns.csv\", header = True, inferSchema = True).withColumnRenamed(\"Order_ID\", \"Returned_Order_ID\")\n",
    "returnInfoDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"Region_1\" among (Order_ID, OrderDate, City, Category, Sub_Category, Sales, Quantitty, Region);'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o79.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"Region_1\" among (Order_ID, OrderDate, City, Category, Sub_Category, Sales, Quantitty, Region);\r\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:216)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:215)\r\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1105)\r\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1075)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-74616a0899a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjoinExpression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuperStoreDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Region_1\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmanagerInfoDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Region\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \"\"\"\n\u001b[0;32m    998\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m             \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"Region_1\" among (Order_ID, OrderDate, City, Category, Sub_Category, Sales, Quantitty, Region);'"
     ]
    }
   ],
   "source": [
    "joinExpression = superStoreDF[\"Region_1\"] == managerInfoDF[\"Region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Order_ID',\n",
       " 'OrderDate',\n",
       " 'City',\n",
       " 'Category',\n",
       " 'Sub_Category',\n",
       " 'Sales',\n",
       " 'Quantitty',\n",
       " 'Region_1',\n",
       " 'Person',\n",
       " 'Region']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superStoreDF1 = superStoreDF.withColumnRenamed(\"Region\",\"Region_1\")\n",
    "superStoreDF1.cache()\n",
    "\n",
    "superStoreDF1.join(managerInfoDF, superStoreDF1[\"Region_1\"] == managerInfoDF[\"Region\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-------+--------+-----------------+\n",
      "| OrderDate|         City|  Sales|Region_1|          manager|\n",
      "+----------+-------------+-------+--------+-----------------+\n",
      "|10/19/2017|      Houston| 29.472| Central|   Kelly Williams|\n",
      "| 12/9/2017|      Houston|  1.248| Central|   Kelly Williams|\n",
      "| 12/9/2017|      Houston|  9.708| Central|   Kelly Williams|\n",
      "| 12/9/2017|      Houston|  27.24| Central|   Kelly Williams|\n",
      "|11/13/2017|      Chicago|230.376| Central|   Kelly Williams|\n",
      "|10/26/2017|    Rochester|  19.99| Central|   Kelly Williams|\n",
      "|10/26/2017|    Rochester|   6.16| Central|   Kelly Williams|\n",
      "| 11/6/2017|     Portland|  5.682|    West|    Anna Andreadi|\n",
      "| 11/9/2017|New York City|  96.53|    East|      Chuck Magee|\n",
      "|11/23/2017|    Charlotte| 74.112|   South|Cassandra Brandow|\n",
      "|11/23/2017|    Charlotte| 27.992|   South|Cassandra Brandow|\n",
      "|11/23/2017|    Charlotte|  3.304|   South|Cassandra Brandow|\n",
      "|12/25/2017|New York City|  41.96|    East|      Chuck Magee|\n",
      "| 11/5/2017|      Phoenix|  2.388|    West|    Anna Andreadi|\n",
      "| 11/5/2017|      Phoenix|243.992|    West|    Anna Andreadi|\n",
      "|12/22/2017| Independence| 839.43| Central|   Kelly Williams|\n",
      "| 11/6/2017|    Westfield|  46.26|    East|      Chuck Magee|\n",
      "|12/17/2017|       Dallas| 66.284| Central|   Kelly Williams|\n",
      "| 12/9/2017|     Whittier|444.768|    West|    Anna Andreadi|\n",
      "| 12/1/2017|      Saginaw|  83.92| Central|   Kelly Williams|\n",
      "+----------+-------------+-------+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column, desc, expr\n",
    "\n",
    "superStoreDF1.join(managerInfoDF, superStoreDF1[\"Region_1\"] == managerInfoDF[\"Region\"] , 'leftouter').select('OrderDate', 'City', 'Sales','Region_1', col('Person').alias(\"manager\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [OrderDate#122, City#123, Sales#126, Region_1#520, Person#155 AS manager#734]\n",
      "+- *BroadcastHashJoin [Region_1#520], [Region#156], LeftOuter, BuildRight\n",
      "   :- *Project [OrderDate#122, City#123, Sales#126, Region#128 AS Region_1#520]\n",
      "   :  +- *FileScan csv [OrderDate#122,City#123,Sales#126,Region#128] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/data/SuperStore/SuperStore_US_Sales.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<OrderDate:string,City:string,Sales:double,Region:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]))\n",
      "      +- *FileScan csv [Person#155,Region#156] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/C:/data/SuperStore/managerByRegion.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Person:string,Region:string>\n"
     ]
    }
   ],
   "source": [
    "superStoreDF1.join(managerInfoDF, superStoreDF1[\"Region_1\"] == managerInfoDF[\"Region\"] , 'leftouter').select('OrderDate', 'City', 'Sales','Region_1', col('Person').alias(\"manager\") ).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-----------+---------------+------------+-------+---------+--------+-----------+\n",
      "|      Order_ID| OrderDate|       City|       Category|Sub_Category|  Sales|Quantitty|Region_1|return_flag|\n",
      "+--------------+----------+-----------+---------------+------------+-------+---------+--------+-----------+\n",
      "|CA-2017-137099| 12/7/2017|Los Angeles|     Technology|      Phones|374.376|        3|    West|       true|\n",
      "|CA-2017-102519|11/27/2017|  Milwaukee|      Furniture| Furnishings|  46.94|        1| Central|       true|\n",
      "|CA-2017-102519|11/27/2017|  Milwaukee|     Technology| Accessories| 143.73|        9| Central|       true|\n",
      "|CA-2017-136539|12/28/2017| Round Rock|Office Supplies|         Art| 27.168|        2| Central|       true|\n",
      "+--------------+----------+-----------+---------------+------------+-------+---------+--------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" identify the returned item \"\"\"\n",
    "join_expr_return = superStoreDF1[\"Order_ID\"] == returnInfoDF[\"Returned_Order_ID\"]\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "returnINDSalesDF = superStoreDF1.join(returnInfoDF, join_expr_return, \"leftOuter\") #.where(\"Returned_Order_ID is not null\")\n",
    "returnINDSalesDF2 = returnINDSalesDF.select(\"*\", expr(\"Returned_Order_ID is not null\").alias(\"return_flag\")).drop(\"Returned_Order_ID\").drop(\"Returned\")\n",
    "\n",
    "returnINDSalesDF2.where(\"return_flag = true\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
